{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe8ELzPGYBAdDQXXew+JKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1heidi/open-data-monitor/blob/main/Open_AI_Sci_Policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# üìä OPEN DATA POLICY MONITOR ‚Äî U.S. ACADEMIC & GOVERNMENT SOURCES\n",
        "# ================================================================\n",
        "#\n",
        "# üß≠ OVERVIEW:\n",
        "# This Google Colab notebook automatically searches for, summarizes,\n",
        "# and reports on recent developments (previous 7 days)in **U.S. open data and open\n",
        "# science policy**, particularly those relevant to academia and\n",
        "# federal agencies.\n",
        "#\n",
        "# The system:\n",
        "#   1. Gathers recent web articles and news (via SERPAPI).\n",
        "#   2. Optionally queries structured feeds (e.g., NASA, NIH, NSF, OSTP, etc.).\n",
        "#   3. Summarizes each finding using OpenAI GPT models, with\n",
        "#   4. Produces a formatted Markdown report (.md) with explicit instructions \"DO NOT fabricate or infer connections\"\n",
        "#        ‚Ä¢ Individual article summaries\n",
        "#        ‚Ä¢ A meta-summary highlighting key policy trends\n",
        "#   5. Saves the report in `/content/open_data_monitor/`\n",
        "#      (or Google Drive if mounted).\n",
        "#   6. Optionally emails the report to you (via Gmail API or SMTP).\n",
        "#\n",
        "# ------------------------------------------------\n",
        "# üìÇ EXPECTED OUTPUTS:\n",
        "# ------------------------------------------------\n",
        "#   ‚Ä¢ Markdown report file:\n",
        "#       e.g., /content/open_data_monitor/open_data_policy_report_YYYYMMDD_HHMM.md\n",
        "#   ‚Ä¢ Console output showing:\n",
        "#       ‚úÖ Collection and summarization progress\n",
        "#       üí∞ Token usage + estimated cost\n",
        "#       ‚ö†Ô∏è Warnings for token limits or missing API keys\n",
        "#\n",
        "# ------------------------------------------------\n",
        "# ‚è±Ô∏è RUNTIME ESTIMATES:\n",
        "# ------------------------------------------------\n",
        "#   ‚Ä¢ Section 3 (collection): ~10‚Äì20 seconds (20 entries typical)\n",
        "#   ‚Ä¢ Section 4 (summarization): ~2‚Äì4 minutes\n",
        "#   ‚Ä¢ Section 5 (report saving/email): <10 seconds\n",
        "#\n",
        "# Total: 2‚Äì5 minutes (depending on entry count and connection speed)\n",
        "#\n",
        "# ------------------------------------------------\n",
        "# üí∞ COST ESTIMATES (OpenAI API):\n",
        "# ------------------------------------------------\n",
        "#   ‚Ä¢ Typical run (20 summaries + meta-summary):\n",
        "#       ‚âà 6,000‚Äì8,000 input tokens\n",
        "#       ‚âà 2,000‚Äì3,000 output tokens\n",
        "#       ‚Üí ~$0.02‚Äì$0.05 using `gpt-4o-mini`\n",
        "#\n",
        "# Adjusting parameters or model choice (e.g., `gpt-4o` or `gpt-3.5-turbo`)\n",
        "# will change cost and speed proportionally.\n",
        "#\n",
        "# CHECK USAGE HERE: https://platform.openai.com/settings/organization/limits\n",
        "# ------------------------------------------------\n",
        "# ‚öôÔ∏è CONFIGURATION NOTES:\n",
        "# ------------------------------------------------\n",
        "# 1Ô∏è‚É£ API KEYS:\n",
        "#     ‚Ä¢ Set your SERPAPI_KEY and OPENAI_API_KEY in Section 2.\n",
        "#     ‚Ä¢ Gmail settings (if emailing) also live in Section 2.\n",
        "#\n",
        "# 2Ô∏è‚É£ TOKEN BUDGET CONTROL:\n",
        "#     ‚Ä¢ The token limit for summarization is set in SECTION 3.\n",
        "#     ‚Ä¢ Look for this line:\n",
        "#           MAX_TOKENS_BUDGET = 12000\n",
        "#       ‚Üí Reduce it to conserve cost or prevent truncation warnings.\n",
        "#       ‚Üí Increase it if you consistently hit truncation and have budget.\n",
        "#\n",
        "# 3Ô∏è‚É£ ADDING SOURCES OR QUERIES:\n",
        "#     ‚Ä¢ Modify the `search_queries` list in SECTION 3.\n",
        "#     ‚Ä¢ Each string represents a Google-style search with logical operators.\n",
        "#\n",
        "# ------------------------------------------------\n",
        "# üß© TROUBLESHOOTING:\n",
        "# ------------------------------------------------\n",
        "#   ‚Ä¢ If \"name 'client' is not defined\": rerun Section 2 to reinitialize API clients.\n",
        "#   ‚Ä¢ If API quota errors appear, check your OpenAI billing page.\n",
        "#   ‚Ä¢ If Gmail auth fails, verify that your credentials are correct or use the Gmail API.\n",
        "#\n",
        "# ------------------------------------------------\n",
        "# üïí SCHEDULING:\n",
        "# ------------------------------------------------\n",
        "#   ‚Ä¢ To run weekly, use Colab‚Äôs \"Schedule notebook\" feature or\n",
        "#     connect via Google Apps Script / GitHub Actions.\n",
        "#\n",
        "# ------------------------------------------------\n",
        "# ‚úçÔ∏è AUTHOR / MAINTAINER:\n",
        "# ------------------------------------------------\n",
        "#   ‚Ä¢ Custom notebook by ChatGPT (GPT-5), configured for monitoring\n",
        "#     U.S. open data policy and academic open science developments.\n",
        "# ================================================================\n"
      ],
      "metadata": {
        "id": "V4jyJOFyaIIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# ‚öôÔ∏è SECTION 1: SETUP & INSTALLS\n",
        "# ===========================================================\n",
        "# Installs dependencies and imports necessary libraries.\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "!pip install -q openai feedparser google-auth-oauthlib google-auth-httplib2 google-api-python-client requests\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import feedparser\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from time import perf_counter, sleep\n",
        "from openai import OpenAI\n",
        "\n",
        "# ‚úÖ Confirm setup\n",
        "print(\"‚úÖ Environment setup complete and libraries imported successfully.\")\n"
      ],
      "metadata": {
        "id": "Nq_Y5B0dN8L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# ‚öôÔ∏è SECTION 2: CONFIGURATION & API KEYS\n",
        "# ===========================================================\n",
        "#\n",
        "# Purpose:\n",
        "#   ‚Ä¢ Define API keys and feature toggles for the notebook\n",
        "#   ‚Ä¢ Configure Gmail sender settings (optional)\n",
        "#   ‚Ä¢ Control token budgeting and cost tracking\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ---------- Part A. OpenAI + SERPAPI Keys ----------\n",
        "\n",
        "# üëá Paste your actual keys between the quotes\n",
        "\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GMAIL_APP_PASSWORD = os.getenv(\"GMAIL_APP_PASSWORD\")\n",
        "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")\n",
        "EMAIL_USER = os.getenv(\"EMAIL_USER\")\n",
        "EMAIL_RECIPIENT = os.getenv(\"EMAIL_RECIPIENT\")\n",
        "\n",
        "# ---------- Part B. SERPAPI Usage (Optional) ----------\n",
        "\n",
        "# Toggle SERPAPI usage (False = RSS only)\n",
        "USE_SERPAPI = True\n",
        "\n",
        "# ---------- Part B. Gmail (Optional ‚Äî Section 5) ----------\n",
        "\n",
        "SEND_EMAIL = True  # toggle emailing on/off\n",
        "\n",
        "# ---------- Part C. Store keys securely in environment ----------\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "if SERPAPI_KEY:\n",
        "    os.environ[\"SERPAPI_KEY\"] = SERPAPI_KEY\n",
        "if GMAIL_APP_PASSWORD:\n",
        "    os.environ[\"GMAIL_APP_PASSWORD\"] = GMAIL_APP_PASSWORD\n",
        "\n",
        "# ---------- Part D. Token Budget + Cost Tracking ----------\n",
        "\n",
        "TOKEN_BUDGET = 20_000  # soft token limit per run\n",
        "\n",
        "# GPT-4 Turbo (approx) pricing\n",
        "COST_PER_1K_INPUT = 0.01\n",
        "COST_PER_1K_OUTPUT = 0.03\n",
        "\n",
        "def estimate_cost(input_tokens, output_tokens):\n",
        "    \"\"\"Estimate total API cost based on token usage.\"\"\"\n",
        "    return round(\n",
        "        (input_tokens / 1000 * COST_PER_1K_INPUT) +\n",
        "        (output_tokens / 1000 * COST_PER_1K_OUTPUT), 4\n",
        "    )\n",
        "\n",
        "# ---------- Part E. Session Metadata ----------\n",
        "\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(f\"‚úÖ Configuration loaded. Run ID: {RUN_ID}\")\n",
        "print(f\"   SERPAPI enabled: {USE_SERPAPI}\")\n",
        "print(f\"   Token budget: {TOKEN_BUDGET:,} tokens\\n\")\n"
      ],
      "metadata": {
        "id": "uV5adPpZN87i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# 3A. SETUP + SOURCES\n",
        "# ===========================================================\n",
        "import feedparser, requests, time, json\n",
        "\n",
        "print(\"‚úÖ Section 3A loaded: Feedparser + Requests imported.\")\n",
        "\n",
        "SOURCES = [\n",
        "    {\"name\": \"White House OSTP Blog\", \"url\": \"https://www.whitehouse.gov/ostp/feed/\"},\n",
        "    {\"name\": \"NSF Science Matters News\", \"url\": \"https://new.nsf.gov/rss.xml\"},\n",
        "    {\"name\": \"NIH Extramural Nexus\", \"url\": \"https://nexus.od.nih.gov/all/feed/\"},\n",
        "    {\"name\": \"DOE Office of Science News\", \"url\": \"https://www.energy.gov/science/listings/office-science-news/rss.xml\"},\n",
        "    {\"name\": \"The Scholarly Kitchen\", \"url\": \"https://scholarlykitchen.sspnet.org/feed/\"},\n",
        "    {\"name\": \"SPARC Open Access News\", \"url\": \"https://sparcopen.org/news/feed/\"},\n",
        "    {\"name\": \"Research Data Alliance Blog\", \"url\": \"https://www.rd-alliance.org/blog/feed\"},\n",
        "    {\"name\": \"OpenAIRE Blog\", \"url\": \"https://www.openaire.eu/feed\"},\n",
        "    {\"name\": \"CODATA Blog\", \"url\": \"https://codata.org/feed/\"},\n",
        "]\n",
        "\n",
        "print(f\"üì° Loaded {len(SOURCES)} RSS sources.\")"
      ],
      "metadata": {
        "id": "7A2of9cCyD8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# 3B. FETCH + PARSE RSS FEEDS (and optional SERPAPI)\n",
        "# ===========================================================\n",
        "# - Collects recent posts from configured RSS feeds\n",
        "# - (Optional) adds SERPAPI search results about \"open data policy\"\n",
        "# - Produces a clean list of dicts: entries = [{title, link, source, summary, published}]\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import hashlib\n",
        "import datetime as dt\n",
        "\n",
        "print(\"üöÄ Section 3B starting...\")\n",
        "\n",
        "# ------------------------------\n",
        "# Helper: Safe extract text fields\n",
        "# ------------------------------\n",
        "def safe_get(entry, key):\n",
        "    try:\n",
        "        val = entry.get(key, \"\")\n",
        "        return val if val else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ------------------------------\n",
        "# Fetch RSS Feeds\n",
        "# ------------------------------\n",
        "entries = []\n",
        "for s in SOURCES:\n",
        "    name = s[\"name\"]\n",
        "    url = s[\"url\"]\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "        count = 0\n",
        "        for e in feed.entries[:10]:  # cap per source for speed\n",
        "            title = safe_get(e, \"title\")\n",
        "            link = safe_get(e, \"link\")\n",
        "            summary = safe_get(e, \"summary\") or safe_get(e, \"description\")\n",
        "            published = safe_get(e, \"published\") or safe_get(e, \"updated\") or \"\"\n",
        "            # Normalize date to ISO\n",
        "            try:\n",
        "                published_dt = dt.datetime(*e.published_parsed[:6])\n",
        "                published = published_dt.strftime(\"%Y-%m-%d\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            # skip if no title or link\n",
        "            if not title or not link:\n",
        "                continue\n",
        "            entries.append({\n",
        "                \"title\": title.strip(),\n",
        "                \"link\": link.strip(),\n",
        "                \"source\": name,\n",
        "                \"summary\": summary.strip(),\n",
        "                \"published\": published,\n",
        "            })\n",
        "            count += 1\n",
        "        print(f\"‚úÖ {name}: {count} entries fetched.\")\n",
        "    except Exception as ex:\n",
        "        print(f\"‚ö†Ô∏è Failed to parse {name}: {ex}\")\n",
        "\n",
        "print(f\"üìö Total entries collected so far: {len(entries)}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Optional: SERPAPI augmentation\n",
        "# ------------------------------\n",
        "if USE_SERPAPI and os.environ.get(\"SERPAPI_KEY\"):\n",
        "    import requests\n",
        "\n",
        "    print(\"üîç Fetching supplemental results from SERPAPI (Google News)...\")\n",
        "    q = \"U.S. open data OR open science policy site:.gov OR site:.edu\"\n",
        "    serp_url = f\"https://serpapi.com/search.json?engine=google_news&q={q}&api_key={os.environ['SERPAPI_KEY']}\"\n",
        "    try:\n",
        "        resp = requests.get(serp_url, timeout=30)\n",
        "        data = resp.json()\n",
        "        for art in data.get(\"news_results\", [])[:10]:\n",
        "            entries.append({\n",
        "                \"title\": art.get(\"title\", \"\"),\n",
        "                \"link\": art.get(\"link\", \"\"),\n",
        "                \"source\": \"SERPAPI\",\n",
        "                \"summary\": art.get(\"snippet\", \"\"),\n",
        "                \"published\": art.get(\"date\", \"\"),\n",
        "            })\n",
        "        print(\"‚úÖ SERPAPI results added.\")\n",
        "    except Exception as ex:\n",
        "        print(f\"‚ö†Ô∏è SERPAPI fetch failed: {ex}\")\n",
        "else:\n",
        "    print(\"üîé SERPAPI skipped (disabled or no API key).\")\n",
        "\n",
        "# ------------------------------\n",
        "# Deduplicate by title hash\n",
        "# ------------------------------\n",
        "seen = set()\n",
        "deduped = []\n",
        "for e in entries:\n",
        "    h = hashlib.md5(e[\"title\"].encode(\"utf-8\")).hexdigest()\n",
        "    if h not in seen:\n",
        "        deduped.append(e)\n",
        "        seen.add(h)\n",
        "\n",
        "entries = deduped\n",
        "print(f\"üßπ Deduplicated. Final entry count: {len(entries)}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Preview a few\n",
        "# ------------------------------\n",
        "print(\"\\nüìã Sample entries:\")\n",
        "for e in entries[:5]:\n",
        "    print(f\"‚Ä¢ {e['source']}: {e['title']} ({e['published']})\")\n",
        "\n",
        "print(\"\\n‚úÖ Section 3B complete ‚Äî entries ready for summarization.\")\n"
      ],
      "metadata": {
        "id": "fPDCjiAwQ_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# üóûÔ∏è SECTION 3C: RSS FEED COLLECTION (with Date Filter)\n",
        "# ===========================================================\n",
        "# Purpose:\n",
        "#   ‚Ä¢ Collects posts from trusted sources (see SOURCES in 3A)\n",
        "#   ‚Ä¢ Filters to include only items published within the last `days_back` days\n",
        "#   ‚Ä¢ Returns a structured list for summarization\n",
        "#\n",
        "# Notes:\n",
        "#   - Default = last 7 days, adjustable by changing `days_back`\n",
        "#   - Feeds missing publication dates are skipped (to avoid false positives)\n",
        "#   - Each entry includes: title, link, summary, source, published_date\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import feedparser\n",
        "\n",
        "def collect_sources(days_back=7):\n",
        "    \"\"\"\n",
        "    Pull top posts from each RSS source published within the last `days_back` days.\n",
        "    Returns list of dicts with title, link, summary, and source.\n",
        "    \"\"\"\n",
        "    entries = []\n",
        "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "    print(f\"üóûÔ∏è Collecting RSS feed updates (last {days_back} days)...\")\n",
        "\n",
        "    for src in SOURCES:\n",
        "        print(f\"‚Üí Fetching {src['name']} ...\")\n",
        "        try:\n",
        "            feed = feedparser.parse(src[\"url\"])\n",
        "            for entry in feed.entries[:10]:  # grab up to 10, then filter\n",
        "                # Try to get published date\n",
        "                published_parsed = entry.get(\"published_parsed\") or entry.get(\"updated_parsed\")\n",
        "                if not published_parsed:\n",
        "                    continue  # skip if no date\n",
        "\n",
        "                published_date = datetime(*published_parsed[:6])\n",
        "                if published_date < cutoff_date:\n",
        "                    continue  # skip older entries\n",
        "\n",
        "                entries.append({\n",
        "                    \"source\": src[\"name\"],\n",
        "                    \"title\": entry.get(\"title\", \"No title\"),\n",
        "                    \"link\": entry.get(\"link\", \"\"),\n",
        "                    \"summary\": entry.get(\"summary\", \"\"),\n",
        "                    \"published\": published_date.strftime(\"%Y-%m-%d\"),\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to parse {src['name']}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ RSS feed collection complete ‚Äî {len(entries)} entries found within {days_back} days.\\n\")\n",
        "    return entries\n",
        "\n",
        "print(\"‚úÖ Section 3B loaded: RSS collector ready.\")\n"
      ],
      "metadata": {
        "id": "zTSpPjpRRBqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# üîç SECTION 3D: SERPAPI SEARCH (Past Week Only)\n",
        "# ===========================================================\n",
        "# Purpose:\n",
        "#   ‚Ä¢ Runs Google searches via SERPAPI for open data / open science policy updates\n",
        "#   ‚Ä¢ Limits results to the past week using tbs=qdr:w\n",
        "#   ‚Ä¢ Returns structured list of recent search hits\n",
        "#\n",
        "# Notes:\n",
        "#   - Respects free-tier (100 searches/month)\n",
        "#   - Each query retrieves up to 3 results\n",
        "#   - Works seamlessly with `collect_sources()` from Section 3B\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# --- Keyword search queries (tuned for policy updates) ---\n",
        "SEARCH_QUERIES = [\n",
        "    \"open science policy site:whitehouse.gov OR site:ostp.gov\",\n",
        "    \"research data sharing policy site:.gov\",\n",
        "    \"federal open data initiative\",\n",
        "    \"open access mandate university research\",\n",
        "    \"data management plan compliance US federal agency\",\n",
        "    \"FAIR data principles higher education US\",\n",
        "    \"open research infrastructure policy America\",\n",
        "    \"OSTP Nelson memo implementation\",\n",
        "    \"open data policy in academia site:.edu\",\n",
        "]\n",
        "\n",
        "def collect_serpapi_results():\n",
        "    \"\"\"\n",
        "    Run SERPAPI searches (past week only).\n",
        "    Only executes if SERPAPI_KEY and USE_SERPAPI are set.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    if not USE_SERPAPI:\n",
        "        print(\"‚öôÔ∏è SERPAPI disabled in config ‚Äî skipping web search.\\n\")\n",
        "        return results\n",
        "    if not SERPAPI_KEY:\n",
        "        print(\"‚ùå SERPAPI_KEY not provided ‚Äî skipping web search.\\n\")\n",
        "        return results\n",
        "\n",
        "    print(\"üîé Running SERPAPI keyword searches (past week)...\")\n",
        "\n",
        "    for query in SEARCH_QUERIES:\n",
        "        try:\n",
        "            print(f\"‚Üí Searching: {query}\")\n",
        "            url = f\"https://serpapi.com/search.json?q={query}&engine=google&api_key={SERPAPI_KEY}&num=3&tbs=qdr:w\"\n",
        "            resp = requests.get(url)\n",
        "\n",
        "            if resp.status_code != 200:\n",
        "                print(f\"‚ö†Ô∏è SERPAPI returned status {resp.status_code} for query '{query}'\")\n",
        "                continue\n",
        "\n",
        "            data = resp.json()\n",
        "            organic_results = data.get(\"organic_results\", [])[:3]\n",
        "\n",
        "            for item in organic_results:\n",
        "                results.append({\n",
        "                    \"source\": \"SERPAPI Google Search\",\n",
        "                    \"title\": item.get(\"title\", \"\"),\n",
        "                    \"link\": item.get(\"link\", \"\"),\n",
        "                    \"summary\": item.get(\"snippet\", \"\"),\n",
        "                    \"published\": \"Past week (SERPAPI)\",\n",
        "                })\n",
        "\n",
        "            time.sleep(2)  # prevent free-tier rate-limit errors\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è SERPAPI query failed for '{query}': {e}\")\n",
        "\n",
        "    print(f\"‚úÖ SERPAPI collection complete ‚Äî {len(results)} results gathered.\\n\")\n",
        "    return results\n",
        "\n",
        "serpapi_results = collect_serpapi_results()\n"
      ],
      "metadata": {
        "id": "iJBdJNEkREFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# 3E. TOKEN BUDGET ESTIMATION\n",
        "# ===========================================================\n",
        "def estimate_token_usage(entries, avg_chars_per_token=4, tokens_per_summary=500, token_budget=TOKEN_BUDGET):\n",
        "    total_chars = sum(len(e.get(\"summary\", \"\")) for e in entries)\n",
        "    estimated_input_tokens = total_chars // avg_chars_per_token\n",
        "    estimated_output_tokens = len(entries) * tokens_per_summary\n",
        "    total_estimated = estimated_input_tokens + estimated_output_tokens\n",
        "\n",
        "    print(\"\\nüßÆ TOKEN BUDGET ESTIMATE\")\n",
        "    print(f\"  Input text: ~{estimated_input_tokens:,} tokens\")\n",
        "    print(f\"  Summaries:  ~{estimated_output_tokens:,} tokens\")\n",
        "    print(f\"  Total est.: ~{total_estimated:,} tokens\")\n",
        "    print(f\"  Budget cap: {token_budget:,} tokens\")\n",
        "\n",
        "    if total_estimated > token_budget:\n",
        "        print(\"‚ö†Ô∏è WARNING: Estimated usage exceeds token budget! Truncating entries.\")\n",
        "        entries = entries[:max(1, token_budget // tokens_per_summary)]\n",
        "        print(f\"‚úÖ Truncated list to {len(entries)} entries.\\n\")\n",
        "    else:\n",
        "        print(\"‚úÖ Within safe token budget.\\n\")\n",
        "\n",
        "    return entries\n",
        "\n",
        "print(\"‚úÖ Section 3D loaded: Token estimation ready.\")\n"
      ],
      "metadata": {
        "id": "J6D_JxPvRG6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# 3F. RUN DATA COLLECTION PIPELINE\n",
        "# ===========================================================\n",
        "def gather_all_sources():\n",
        "    start = time.time()\n",
        "    print(\"\\nüöÄ Starting data collection pipeline...\\n\")\n",
        "\n",
        "    rss_entries = collect_sources()\n",
        "    print(f\"üïí RSS collection done in {time.time() - start:.2f}s.\\n\")\n",
        "\n",
        "    serp_entries = collect_serpapi_results()\n",
        "    print(f\"üïí SERPAPI search done in {time.time() - start:.2f}s.\\n\")\n",
        "\n",
        "    all_entries = rss_entries + serp_entries\n",
        "    print(f\"üì¶ Combined {len(all_entries)} total entries.\\n\")\n",
        "\n",
        "    all_entries = estimate_token_usage(all_entries, token_budget=TOKEN_BUDGET)\n",
        "    print(f\"üèÅ Total runtime: {time.time() - start:.2f}s\\n\")\n",
        "\n",
        "    return all_entries\n",
        "\n",
        "print(\"‚úÖ Section 3E loaded: Ready to execute full pipeline.\")\n"
      ],
      "metadata": {
        "id": "MFlOgJ94RKhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# 3G. MANUAL EXECUTION\n",
        "# ===========================================================\n",
        "print(\"\\n=== RUNNING SECTION 3 ===\\n\")\n",
        "entries = gather_all_sources()\n",
        "print(f\"\\n‚úÖ Done! Gathered {len(entries)} entries total.\\n\")\n"
      ],
      "metadata": {
        "id": "Lh9WSxy4RMxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# üß† SECTION 4: Summaries + Meta-Summary WITH NUMBERED REFERENCES\n",
        "# ===========================================================\n",
        "# - Produces meta-summary bullets and deterministically maps each bullet\n",
        "#   to supporting sources (title + numbered reference) drawn from the collected entries.\n",
        "# - Only includes numbered references (no inline links).\n",
        "# - Output sections:\n",
        "#     1. META-SUMMARY WITH NUMBERED REFERENCES\n",
        "#     2. DETAILED SUMMARIES (with numbered references, no links)\n",
        "#     3. REFERENCES (unique supporting sources)\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "import openai\n",
        "\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)  # ‚úÖ uses your existing API key variable\n",
        "\n",
        "# Config (will use globals if set)\n",
        "MODEL = globals().get(\"MODEL_NAME\", \"gpt-3.5-turbo\")\n",
        "PER_SUMMARY_MAX_TOKENS = globals().get(\"PER_SUMMARY_MAX_TOKENS\", 400)\n",
        "SUMMARY_PAUSE = globals().get(\"SUMMARY_PAUSE\", 0.8)\n",
        "META_MAX_TOKENS = 400\n",
        "USE_AI_FOR_INDIVIDUAL_SUMMARIES = True\n",
        "SAVE_TO_FILE = False\n",
        "\n",
        "_STOPWORDS = {\n",
        "    \"the\",\"and\",\"for\",\"that\",\"with\",\"from\",\"this\",\"which\",\"their\",\"have\",\"has\",\"are\",\"was\",\n",
        "    \"were\",\"will\",\"would\",\"could\",\"should\",\"about\",\"within\",\"between\",\"among\",\"these\",\"those\",\n",
        "    \"such\",\"also\",\"not\",\"new\",\"use\",\"used\",\"using\",\"may\",\"can\",\"us\",\"our\",\"more\",\"most\",\"over\"\n",
        "}\n",
        "\n",
        "def _normalize_text(t):\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    t = unicodedata.normalize(\"NFKC\", str(t))\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def _tokenize_for_matching(t):\n",
        "    t = _normalize_text(t).lower()\n",
        "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n",
        "    words = [w for w in t.split() if len(w) >= 4 and w not in _STOPWORDS]\n",
        "    return words\n",
        "\n",
        "def _ai_summarize_entry(text, source, link):\n",
        "    text = _normalize_text(text)\n",
        "    if not text:\n",
        "        return \"(No text available.)\"\n",
        "    cli = globals().get(\"client\", None)\n",
        "    if not cli:\n",
        "        return f\"[OFFLINE] {source}: {text[:280]}...\"\n",
        "    prompt = (\n",
        "        f\"Summarize this recent update from {source} about U.S. open data or open science policy. \"\n",
        "        \"Only use facts present in the text. DO NOT MAKE UP facts, dates, or sources.\\n\\n\"\n",
        "        f\"Source link: {link}\\n\\nText:\\n{text[:4000]}\"\n",
        "    )\n",
        "    try:\n",
        "        resp = cli.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            max_tokens=PER_SUMMARY_MAX_TOKENS,\n",
        "        )\n",
        "        s = resp.choices[0].message.content.strip()\n",
        "        time.sleep(SUMMARY_PAUSE)\n",
        "        return s\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è AI summarization failed for source {source}: {e}\")\n",
        "        return f\"[OFFLINE] {source}: {text[:280]}...\"\n",
        "\n",
        "def _ai_meta_bullets(summaries_list):\n",
        "    cli = globals().get(\"client\", None)\n",
        "    if not cli:\n",
        "        return \"\"\n",
        "    prompt = (\n",
        "        \"You are a concise policy analyst. From the following short summaries of recent updates \"\n",
        "        \"about U.S. open data / open science policy, produce exactly FIVE numbered, concise key \"\n",
        "        \"takeaways (one idea per bullet, 1‚Äì2 sentences each). Use only information present in the summaries. \"\n",
        "        \"Do NOT fabricate or infer missing facts. Return the 5 bullets only.\\n\\n\"\n",
        "        \"Summaries:\\n\\n\" + \"\\n\\n\".join(summaries_list)\n",
        "    )\n",
        "    try:\n",
        "        resp = cli.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            max_tokens=META_MAX_TOKENS,\n",
        "        )\n",
        "        return resp.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Meta-summary API call failed:\", e)\n",
        "        return \"\"\n",
        "\n",
        "def _parse_numbered_bullets(text):\n",
        "    if not text:\n",
        "        return []\n",
        "    lines = text.splitlines()\n",
        "    bullets = []\n",
        "    pattern = re.compile(r'^\\s*(?:\\d+\\.)\\s*(.+)$')\n",
        "    for ln in lines:\n",
        "        m = pattern.match(ln)\n",
        "        if m:\n",
        "            bullets.append(m.group(1).strip())\n",
        "    if bullets:\n",
        "        return bullets\n",
        "    for ln in lines:\n",
        "        ln2 = ln.strip()\n",
        "        if ln2.startswith('- ') or ln2.startswith('* '):\n",
        "            bullets.append(ln2[2:].strip())\n",
        "    if not bullets:\n",
        "        parts = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n",
        "        return parts[:5]\n",
        "    return bullets[:5]\n",
        "\n",
        "def _match_bullet_to_sources(bullet, items, top_k=5):\n",
        "    btoks = set(_tokenize_for_matching(bullet))\n",
        "    scores = []\n",
        "    for it in items:\n",
        "        title = it.get(\"title\",\"\")\n",
        "        summary = it.get(\"summary\",\"\")\n",
        "        source = it.get(\"source\",\"\")\n",
        "        candidate_text = \" \".join([title, summary, source])\n",
        "        ctoks = set(_tokenize_for_matching(candidate_text))\n",
        "        overlap = btoks & ctoks\n",
        "        score = len(overlap)\n",
        "        title_toks = set(_tokenize_for_matching(title))\n",
        "        score += len(btoks & title_toks)\n",
        "        scores.append((score, it))\n",
        "    scored = [it for score,it in sorted(scores, key=lambda x: x[0], reverse=True) if score>0]\n",
        "    return scored[:top_k]\n",
        "\n",
        "def generate_report_with_refs(entries, use_ai_for_individual=USE_AI_FOR_INDIVIDUAL_SUMMARIES):\n",
        "    if not entries:\n",
        "        print(\"‚ùå No entries provided to generate_report_with_refs().\")\n",
        "        return \"\", {}\n",
        "\n",
        "    t0 = time.time()\n",
        "    print(f\"üß† Generating summaries for {len(entries)} entries...\")\n",
        "\n",
        "    items = []\n",
        "    for e in entries:\n",
        "        title = e.get(\"title\",\"(no title)\")\n",
        "        link = e.get(\"link\",\"\")\n",
        "        source = e.get(\"source\",\"\")\n",
        "        raw_text = e.get(\"summary\",\"\") or e.get(\"content\",\"\") or \"\"\n",
        "        summary = _ai_summarize_entry(raw_text, source, link) if use_ai_for_individual else raw_text or \"(no text)\"\n",
        "        items.append({\n",
        "            \"title\": _normalize_text(title),\n",
        "            \"link\": _normalize_text(link),\n",
        "            \"source\": _normalize_text(source),\n",
        "            \"summary\": _normalize_text(summary),\n",
        "            \"published\": e.get(\"published\",\"\")\n",
        "        })\n",
        "\n",
        "    s_texts = [f\"[{it['source']}] {it['title']}: {it['summary']}\" for it in items]\n",
        "    meta_raw = _ai_meta_bullets(s_texts)\n",
        "    bullets = _parse_numbered_bullets(meta_raw)\n",
        "\n",
        "    if not bullets:\n",
        "        print(\"‚ö†Ô∏è Model did not produce numbered bullets ‚Äî using heuristic fallback.\")\n",
        "        all_tokens = []\n",
        "        for it in items:\n",
        "            all_tokens += _tokenize_for_matching(it['summary'])\n",
        "        common = [w for w,_ in Counter(all_tokens).most_common(10)]\n",
        "        bullets = [f\"Frequent theme: {w}\" for w in common[:5]]\n",
        "\n",
        "    print(f\"üîé Parsed {len(bullets)} meta bullets. Mapping bullets to supporting sources...\")\n",
        "\n",
        "    bullet_mappings = []\n",
        "    all_cited = {}\n",
        "    ref_counter = 1\n",
        "    ref_numbers = {}\n",
        "\n",
        "    for b in bullets:\n",
        "        matched = _match_bullet_to_sources(b, items, top_k=6)\n",
        "        formatted_nums = []\n",
        "        for m in matched:\n",
        "            title = m.get(\"title\",\"(no title)\")\n",
        "            link = m.get(\"link\",\"\")\n",
        "            if title not in ref_numbers:\n",
        "                ref_numbers[title] = ref_counter\n",
        "                all_cited[ref_counter] = (title, link)\n",
        "                ref_counter += 1\n",
        "            formatted_nums.append(f\"[{ref_numbers[title]}]\")\n",
        "        bullet_mappings.append((b, formatted_nums))\n",
        "\n",
        "    # --- Report Construction ---\n",
        "    timestamp = datetime.now().strftime(\"%B %d, %Y\")\n",
        "    header = f\"OPEN DATA POLICY MONITOR REPORT\\nGenerated on {timestamp}\\n\\n\"\n",
        "\n",
        "    # === META-SUMMARY WITH NUMBERED REFERENCES ===\n",
        "    header += \"============================================================\\n\"\n",
        "    header += \"üîó META-SUMMARY WITH NUMBERED REFERENCES\\n\"\n",
        "    header += \"============================================================\\n\\n\"\n",
        "\n",
        "    meta_block = \"\"\n",
        "    for i,(b,refs) in enumerate(bullet_mappings, start=1):\n",
        "        ref_str = \" \".join(refs) if refs else \"(no match)\"\n",
        "        meta_block += f\"{i}. {b} {ref_str}\\n\\n\"\n",
        "\n",
        "   # === DETAILED SUMMARIES ===\n",
        "    details_block = \"\\n\\n============================================================\\n\"\n",
        "    details_block += \"üìñ DETAILED SUMMARIES (only entries supporting meta-summary bullets)\\n\"\n",
        "    details_block += \"============================================================\\n\\n\"\n",
        "    for it in items:\n",
        "        title = it['title']\n",
        "        source = it['source']\n",
        "        link = it['link']\n",
        "        ref_num = ref_numbers.get(title, None)\n",
        "\n",
        "        # ‚úÖ Skip entries that don't have a reference number\n",
        "        if not ref_num:\n",
        "            continue\n",
        "\n",
        "        details_block += f\"TITLE: {title} [{ref_num}]\\nSOURCE: {source}\\n\\n{it['summary']}\\n\"\n",
        "        details_block += \"-\"*60 + \"\\n\"\n",
        "\n",
        "    # === REFERENCES (unique supporting sources) ===\n",
        "    refs_block = \"\\n============================================================\\n\"\n",
        "    refs_block += \"üìö REFERENCES (unique supporting sources)\\n\"\n",
        "    refs_block += \"============================================================\\n\\n\"\n",
        "    if all_cited:\n",
        "        for num, (title, link) in all_cited.items():\n",
        "            if link:\n",
        "                refs_block += f\"{num}. {title} ‚Äî {link}\\n\"\n",
        "            else:\n",
        "                refs_block += f\"{num}. {title}\\n\"\n",
        "    else:\n",
        "        refs_block += \"(No sources were matched to bullets.)\\n\"\n",
        "\n",
        "    footer = \"\\n\" + \"=\"*60 + \"\\n\"\n",
        "    runtime = round(time.time() - t0, 2)\n",
        "    total_in = sum(len(it['summary'])//4 for it in items)\n",
        "    total_out = sum((len(b)//4) for b in bullets) + 200\n",
        "    est_cost = round((total_in/1000)*0.01 + (total_out/1000)*0.03, 4)\n",
        "    footer += f\"RUNTIME: {runtime} s\\nESTIMATED TOKENS: {total_in + total_out}\\nESTIMATED COST (USD): ${est_cost}\\n\"\n",
        "    footer += \"=\"*60 + \"\\n\"\n",
        "\n",
        "    report_text = header + meta_block + details_block + refs_block + footer\n",
        "\n",
        "   if SAVE_TO_FILE:\n",
        "    # Use /content path if running in Colab, otherwise save to local \"reports\" folder\n",
        "    if os.path.exists(\"/content\"):\n",
        "        save_dir = \"/content/open_data_monitor\"\n",
        "    else:\n",
        "        save_dir = \"reports\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fname = os.path.join(save_dir, f\"open_data_policy_report_{datetime.now().strftime('%Y%m%d')}.txt\")\n",
        "\n",
        "    try:\n",
        "        with open(fname, \"w\", encoding=\"utf-8\") as fh:\n",
        "            fh.write(report_text)\n",
        "        print(f\"üíæ Report saved to {fname}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not save report: {e}\")\n",
        "\n",
        "    stats = {\n",
        "        \"entries\": len(entries),\n",
        "        \"bullets\": len(bullets),\n",
        "        \"runtime_s\": runtime,\n",
        "        \"estimated_cost_usd\": est_cost,\n",
        "    }\n",
        "    print(\"\\n‚úÖ Section 4 complete ‚Äî meta-summary, detailed summaries, and references generated.\")\n",
        "    return report_text, stats\n",
        "\n",
        "# -------------------- Run now (if entries exist) --------------------\n",
        "if \"entries\" in globals() and entries:\n",
        "    report_text, section4_stats = generate_report_with_refs(entries, use_ai_for_individual=USE_AI_FOR_INDIVIDUAL_SUMMARIES)\n",
        "    print(\"\\n--- META-SUMMARY PREVIEW ---\\n\")\n",
        "    print(\"\\n\".join(report_text.splitlines()[:30]))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è `entries` not found ‚Äî run Section 3 first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpmgm3-no-Af",
        "outputId": "81e17fa3-087e-4c3d-8f3f-6a194e5c17a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Generating summaries for 34 entries...\n",
            "üîé Parsed 5 meta bullets. Mapping bullets to supporting sources...\n",
            "\n",
            "‚úÖ Section 4 complete ‚Äî meta-summary, detailed summaries, and references generated.\n",
            "\n",
            "--- META-SUMMARY PREVIEW ---\n",
            "\n",
            "OPEN DATA POLICY MONITOR REPORT\n",
            "Generated on October 10, 2025\n",
            "\n",
            "============================================================\n",
            "üîó META-SUMMARY WITH NUMBERED REFERENCES\n",
            "============================================================\n",
            "\n",
            "1. AI is reshaping academic research economics, highlighting the importance of considerations like privacy and sustainability. [1] [2] [3] [4] [5] [6]\n",
            "\n",
            "2. Political challenges pose risks to open data collections and emphasize the need for preserving cross-disciplinary information. [7] [8] [9] [10] [11] [12]\n",
            "\n",
            "3. Unauthorized AI web harvesting bots present significant management issues for content-rich websites. [13] [3] [14] [15] [16] [17]\n",
            "\n",
            "4. Writing a manifesto can mobilize action and promote team building within publishing organizations. [18] [9] [4] [19] [20] [7]\n",
            "\n",
            "5. Open data and open science policies aim to improve transparency and interaction in the U.S. government-citizen ecosystem. [12] [9] [8] [21] [11] [21]\n",
            "\n",
            "\n",
            "\n",
            "============================================================\n",
            "üìñ DETAILED SUMMARIES (only entries supporting meta-summary bullets)\n",
            "============================================================\n",
            "\n",
            "TITLE: Guest Post ‚Äî The Economics of AI in Academic Research [1]\n",
            "SOURCE: The Scholarly Kitchen\n",
            "\n",
            "The recent update from The Scholarly Kitchen discusses how AI will reshape the economics of academic research, which is considered the most misunderstood issue in the fast-moving world of AI research tools. The update emphasizes the importance of vendors having strong opinions and plans for various community-focused concerns in AI research, such as privacy, security, sustainability, and copyright.\n",
            "------------------------------------------------------------\n",
            "TITLE: Guest Post ‚Äî Rethinking Disciplinary Data Regimes [7]\n",
            "SOURCE: The Scholarly Kitchen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# üì® SECTION 5: Save + Email the Full Report\n",
        "# ===========================================================\n",
        "# Saves the final report to a date-stamped .txt file and emails it\n",
        "# using Gmail SMTP (requires an App Password, not your normal password).\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import smtplib\n",
        "import ssl\n",
        "from datetime import datetime\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "\n",
        "# Mailing\n",
        "EMAIL_SUBJECT = \"üìä Open Data Policy Monitor Weekly Report\"\n",
        "SAVE_DIR = \"/content/open_data_monitor\"\n",
        "\n",
        "def save_and_email_report(report_text):\n",
        "    \"\"\"Save the report locally and send it via Gmail.\"\"\"\n",
        "    if not report_text:\n",
        "        print(\"‚ùå No report_text found ‚Äî run Section 4 first.\")\n",
        "        return\n",
        "\n",
        "    # --- Save the report ---\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "    # Use only Month-Day-Year for both file name and report header\n",
        "    timestamp = datetime.now().strftime(\"%B-%d-%Y\")       # e.g., October-09-2025\n",
        "    readable_stamp = datetime.now().strftime(\"%B %d, %Y\")  # e.g., October 09, 2025\n",
        "\n",
        "    filename = os.path.join(SAVE_DIR, f\"open_data_policy_report_{timestamp}.txt\")\n",
        "\n",
        "    # Prepend readable timestamp to the top of the report\n",
        "    header = f\"# Open Data Policy Monitor\\nüóìÔ∏è Generated on {readable_stamp}\\n\\n\"\n",
        "    full_report = header + report_text\n",
        "\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(full_report)\n",
        "\n",
        "    print(f\"üíæ Report saved successfully:\\n   {filename}\")\n",
        "\n",
        "    # --- Prepare email ---\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = GMAIL_ADDRESS\n",
        "    msg[\"To\"] = EMAIL_RECIPIENT\n",
        "    msg[\"Subject\"] = EMAIL_SUBJECT\n",
        "\n",
        "    body = (\n",
        "        f\"Attached is the latest Open Data Policy Monitor report.\\n\\n\"\n",
        "        f\"üóìÔ∏è Generated on {readable_stamp}\\n\\n\"\n",
        "        \"The full text is also included below.\\n\\n\"\n",
        "        f\"{report_text}\"\n",
        "    )\n",
        "    msg.attach(MIMEText(body, \"plain\", \"utf-8\"))\n",
        "\n",
        "    # --- Send email via Gmail SMTP ---\n",
        "    try:\n",
        "        context = ssl.create_default_context()\n",
        "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
        "            server.login(GMAIL_ADDRESS, GMAIL_APP_PASSWORD)\n",
        "            server.send_message(msg)\n",
        "        print(f\"üì® Email successfully sent to {EMAIL_RECIPIENT}\")\n",
        "    except smtplib.SMTPAuthenticationError:\n",
        "        print(\"‚ö†Ô∏è Gmail rejected your login ‚Äî check your App Password or 2FA settings.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to send email: {e}\")\n",
        "\n",
        "# --- Run the function ---\n",
        "if \"report_text\" in globals() and report_text:\n",
        "    save_and_email_report(report_text)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No report_text variable found. Please run Section 4 first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb2FDGZipDv5",
        "outputId": "cfe74871-8289-4b7c-f4a9-f80d00a5de66"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Report saved successfully:\n",
            "   /content/open_data_monitor/open_data_policy_report_October-10-2025.txt\n",
            "üì® Email successfully sent to imker@illinois.edu\n"
          ]
        }
      ]
    }
  ]
}